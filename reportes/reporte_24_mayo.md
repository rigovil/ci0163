## Resumen de los temas estudiados
Una vez que un modelo de *machine learning* haya sido creado, a partir de los distintos algoritmos existentes, se debe entrenar el modelo y evaluarlo. Es una etapa necesaria y existen distintas formas de evaluar. Una es **validación cruzada**, donde el dataset se divide en bloques y se entrenan varios modelos con las distintas particiones del dataset creadas. La idea de fondo es que un modelo sea entrenado y validado con distintos datos, para que arroje resultados más acertados. Dentro de este tipo existen algunas técnicas tales como **k-fold**, donde puede variar su forma, una útil es cuando los datos se pueden dividir en clases y cada iteración va siendo entrenada y evaluando con distintos subconjuntos de los datos dentro de cada clase, lo que produce resultados robustos. Otras técnicas son **LOOCV**, **shuffle split**, **por grupos**, donde la diferencia está en que sirven para menor cantidad de datos, mayor cantidad de datos y cuando los datos provienen de una misma fuente, respectivamente. 

Al momento de parametrizar un modelo, se puede hacer uso de  la técnica de **nested cross validation**, que consiste en dos ciclos donde uno realiza la búsqueda de la mejor combinación de parámetros y el otro crea el modelo utilizando los parámetros ya seleccionados.

Cuando un modelo ha producido resultados, es necesario medirlos para tener más claro la exactitud con la que realiza predicciones. Para ello existe la **matriz de confusión**, donde una entrada es la cantidad de predicciones de verdaderos positivos y negativos, y falsos positivos y negativos. Existen algunas métricas de desempeño numéricas como **TPR** el cual corresponde a la tasa de positivos acertados correctamente dentro de todos los positivos presentes en el dataset (también conocido como **recall**), **TNR** lo mismo pero para los negativos. **FPR** y **FNR** que corresponden a predicciones erróneas del modelo. Otras métricas son **accuracy** y **precisión**, que son contrarias entre una y otra, es decir, el modelo mejora una métrica mientras se deteriora en la otra y viceversa. Una de las métricas más confiables es el **f1 score** el cual es una media armónica entre **precisión** y el **recall**.

Regresando a los algoritmos para crear modelos, existen algunas medidas de regularización que se pueden realizar, por ejemplo, a modelos lineales, donde se agrega una variable a la función de costo, que puede calcularse de distintas maneras: **l1**, **l2** o la combinación de ambas. Se puede elegir dependiendo de las circunstancias, si hay correlación entre atributos, o si algunos aportan poco o nada, etc.

Otro de los algoritmos son las **redes neuronales**, la cual consiste en capas y donde cada capa está formada por una cantidad de **perceptrones** o neuronas. El algoritmo inicia en una capa de entrada, la cual tiene tantos perceptrones como *inputs* tenga el modelo, y secuencialmente va avanzando capa por capa. Cada capa recibe de la capa anterior los resultados de distintos cálculos. Estos resultados utilizan un valor de **peso** el cual es la 'conexión' entre las neuronas de una capa con la capa anterior y posterior. Los cálculos dependen de la **función de activación**, que es previamente definida y utilizada en toda la red. Se puede ver como resultados enviándose de una capa a otra secuencialmente, hasta llegar a una capa de salida, la cual contiene la predicción realizada por el modelo. Las capas del intermedio son capas ocultas, y su cantidad y tamaño (número de neuronas) varía de un modelo a  otro.

## Comentarios

Es bastante útil conocer que existen diferentes métricas de desempeño que permiten hacer comparaciones de un modelo a otro, aunque en el fondo utilicen métodos completamente distintos entre sí, pero que gracias a las métricas se pueden realizar comparaciones objetivas de la efectividad de uno u otro. Me pareció importante saber que un modelo debe entrenarse, validarse y probarse con distintos subconjuntos del dataset para que los modelos no se adecúen a los datos y que se comporten mejor cuando reciben datos nuevos o desconocidos. Las redes neuronales me parecen una técnica bastante útil que aprovecha mejor las facilidades que ofrecen las computadores actuales para realizar cálculos rápidos y precisos.

## Dudas

Ya he tenido la oportunidad de trabajar antes con redes neuronales y nunca supe entender en qué se puede basar alguien para crear la cantidad de capas intermedias más adecuada, así como la cantidad adecuada de neuronas que puede tener cada capa. Muchas veces lo hice a prueba y error pero desconozco si existen heurísticas que permitan facilitar ese proceso. También he escuchado mucho por parte de profesores y otras personas, que se tiende a creer que las redes neuronales pueden solucionar cualquier problemas o que se pueden adaptar a cualquier circunstancia  pero que realmente no es así, ¿por qué no y en qué contexto no son útiles?

## Posible uso como profesional y problemas a resolver

Me ha tocado utilizar redes neuronales para crear personajes de videojuegos que tomen decisiones dependiendo del contexto en el que se encuentran en una partida, en un futuro podría utilizar estos conocimientos que pude ampliar para mejorar el proceso de programación. Puedo utilizar también las distintas métricas de desempeño para evaluar modelos que estén realizando predicciones sobre un mismo problema: bancarios, toma de decisiones, clasificación, etc.
