{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"reporte_tutorial_3.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMd0/4KOqwNV0Z21d+ob+vW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"e3LvOJ4J3z0i"},"outputs":[],"source":["# Feature Selection with Univariate Statistical Tests\n","from pandas import read_csv\n","from numpy import set_printoptions\n","from sklearn.feature_selection import SelectKBest\n","from sklearn.feature_selection import f_classif\n","# load data\n","filename = 'pima-indians-diabetes.data.csv'\n","names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n","dataframe = read_csv(filename, names=names)\n","array = dataframe.values\n","X = array[:,0:8]\n","Y = array[:,8]\n","# feature extraction\n","test = SelectKBest(score_func=f_classif, k=4)\n","fit = test.fit(X, Y)\n","# summarize scores\n","set_printoptions(precision=3)\n","print(fit.scores_)\n","features = fit.transform(X)\n","# summarize selected features\n","print(features[0:5,:])"]},{"cell_type":"code","source":["# Feature Extraction with RFE\n","from pandas import read_csv\n","from sklearn.feature_selection import RFE\n","from sklearn.linear_model import LogisticRegression\n","# load data\n","url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv\"\n","names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n","dataframe = read_csv(url, names=names)\n","array = dataframe.values\n","X = array[:,0:8]\n","Y = array[:,8]\n","# feature extraction\n","model = LogisticRegression(solver='lbfgs')\n","rfe = RFE(model, 3)\n","fit = rfe.fit(X, Y)\n","print(\"Num Features: %d\" % fit.n_features_)\n","print(\"Selected Features: %s\" % fit.support_)\n","print(\"Feature Ranking: %s\" % fit.ranking_)"],"metadata":{"id":"D0AEK-cT7jOh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Feature Extraction with PCA\n","import numpy\n","from pandas import read_csv\n","from sklearn.decomposition import PCA\n","# load data\n","url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv\"\n","names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n","dataframe = read_csv(url, names=names)\n","array = dataframe.values\n","X = array[:,0:8]\n","Y = array[:,8]\n","# feature extraction\n","pca = PCA(n_components=3)\n","fit = pca.fit(X)\n","# summarize components\n","print(\"Explained Variance: %s\" % fit.explained_variance_ratio_)\n","print(fit.components_)"],"metadata":{"id":"uBc4eAvb7i_E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Feature Importance with Extra Trees Classifier\n","from pandas import read_csv\n","from sklearn.ensemble import ExtraTreesClassifier\n","# load data\n","url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv\"\n","names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n","dataframe = read_csv(url, names=names)\n","array = dataframe.values\n","X = array[:,0:8]\n","Y = array[:,8]\n","# feature extraction\n","model = ExtraTreesClassifier(n_estimators=10)\n","model.fit(X, Y)\n","print(model.feature_importances_)"],"metadata":{"id":"oA4khsx-9qKJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Comentarios del ejercicio**"],"metadata":{"id":"6SrkADjR7eG_"}},{"cell_type":"markdown","source":["Ya en ejercicios anteriores se vio como la presencia o ausencia de ciertas variables en el dataset puede afectar el rendimiento de un modelo predictivo, precisamente saber cuáles variables son de mayor importancia es un proceso sumamente importante que puede mejorar los resultados. Realizar un *feature selection* puede prevenir el *overfitting* y mejorar tiempos de entrenamiento. Tal como lo menciona el blog, existen distintas técnicas que permitan realizar un *feature selection* de manera automática con ayuda de la estadística, de ahí salen algunas técnicas como RFE y PCA. En resumen, todas intentan darle una calificación a los atributos/variables de un dataset, probando distintas combinaciones de ellos y obtener aquellos que sean de mayor importancia para lograr buenos resultados con el modelo predictivo, algo que en un futuro puede ayudar en investigaciones donde se traten con datasets muy complejos y con muchos atributos."],"metadata":{"id":"7C22waQn7iQh"}}]}