## Resumen

La función de activación sirve para que las neuronas transformen los inputs, pesos y bias en un resultado que es pasado a las neuronas de la siguiente capa. Hay distintas funciones y la elección depende del tipo de resultado que se está buscando, así como de la arquitectura de la red, la mayoría son funciones no lineales. Las capas intermedias suelen usar la misma función de activación, la capa de salida podría usar una distinta para realizar la predicción. Algunas de las funciones de activación son: **ReLU**, **sigmoide**, **tangente hiperbólica**, etc. La primera de ellas toma el valor del input, si es negativo devuelve 0, si es positivo entonces devuelve el mismo valor, tiene algunas complicaciones aunque es fácil de implementar. La segunda, la función **sigmoide** es la utiliza en regresión logística y devuelve valores entre 0 y 1. La **tangente hiperbólica** y también devuelve valores entre 0 y 1. Para la capa intermedia, las redes neuronales convolucionales suelen utilizar la función **ReLU**, las redes recurrentes la función **tanh** o **sigmoide**, las redes de perceptrones multicapa utilizan **ReLU**. Para la capa de salida, se puede utilizar la función **lineal** si el problema es de regresión. Para clasificaciones binarias y multiclase (incluyentes) se utiliza la **sigmoide**, para clasificaciones multiclase (excluyentes) la función **softmax**.

## Comentarios y Dudas

Tener heurísticas que permitan decidir cuál función de activación utilizar ayuda mucho a simplificar la programación de una red neuronal si se hiciera desde cero. Aún así, puede ayudar al proceso de entrenamiento pues se están utilizando las funciones adecuadas para el tipo de datos que se está usando para entrenar la red. Me queda la duda de por qué se suele cambiar la función de activación entre las capas intermedias y la capa de salida.