{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wb8owLjI3kb5",
        "outputId": "f6a076ba-30ff-4242-c3b7-8ebfb071615d"
      },
      "outputs": [],
      "source": [
        "# summarize the number of unique values for each column using numpy\n",
        "from urllib.request import urlopen\n",
        "from numpy import loadtxt\n",
        "from numpy import unique\n",
        "# define the location of the dataset\n",
        "path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/oil-spill.csv'\n",
        "# load the dataset\n",
        "data = loadtxt(urlopen(path), delimiter=',')\n",
        "# summarize the number of unique values in each column\n",
        "for i in range(data.shape[1]):\n",
        "\tprint(i, len(unique(data[:, i])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqazghO73ycR",
        "outputId": "89eaa372-8d20-40a6-f327-468a8a4a939b"
      },
      "outputs": [],
      "source": [
        "# summarize the number of unique values for each column using numpy\n",
        "from pandas import read_csv\n",
        "# define the location of the dataset\n",
        "path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/oil-spill.csv'\n",
        "# load the dataset\n",
        "df = read_csv(path, header=None)\n",
        "# summarize the number of unique values in each column\n",
        "print(df.nunique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apzcuQGL39dx",
        "outputId": "d794756c-16a4-4646-954b-391d99bae210"
      },
      "outputs": [],
      "source": [
        "# delete columns with a single unique value\n",
        "from pandas import read_csv\n",
        "# define the location of the dataset\n",
        "path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/oil-spill.csv'\n",
        "# load the dataset\n",
        "df = read_csv(path, header=None)\n",
        "print(df.shape)\n",
        "# get number of unique values for each column\n",
        "counts = df.nunique()\n",
        "# record columns to delete\n",
        "to_del = [i for i,v in enumerate(counts) if v == 1]\n",
        "print(to_del)\n",
        "# drop useless columns\n",
        "df.drop(to_del, axis=1, inplace=True)\n",
        "print(df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFzD69U_4ZtO",
        "outputId": "83fb1ae9-ff10-4b40-a335-5aa8a0973786"
      },
      "outputs": [],
      "source": [
        "# summarize the percentage of unique values for each column using numpy\n",
        "from urllib.request import urlopen\n",
        "from numpy import loadtxt\n",
        "from numpy import unique\n",
        "# define the location of the dataset\n",
        "path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/oil-spill.csv'\n",
        "# load the dataset\n",
        "data = loadtxt(urlopen(path), delimiter=',')\n",
        "# summarize the number of unique values in each column\n",
        "for i in range(data.shape[1]):\n",
        "\tnum = len(unique(data[:, i]))\n",
        "\tpercentage = float(num) / data.shape[0] * 100\n",
        "\tprint('%d, %d, %.1f%%' % (i, num, percentage))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUN6gIau4lgf",
        "outputId": "f34e8b57-3392-4ebb-ee8d-84d7738fcb50"
      },
      "outputs": [],
      "source": [
        "# summarize the percentage of unique values for each column using numpy\n",
        "from urllib.request import urlopen\n",
        "from numpy import loadtxt\n",
        "from numpy import unique\n",
        "# define the location of the dataset\n",
        "path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/oil-spill.csv'\n",
        "# load the dataset\n",
        "data = loadtxt(urlopen(path), delimiter=',')\n",
        "# summarize the number of unique values in each column\n",
        "for i in range(data.shape[1]):\n",
        "\tnum = len(unique(data[:, i]))\n",
        "\tpercentage = float(num) / data.shape[0] * 100\n",
        "\tif percentage < 1:\n",
        "\t\tprint('%d, %d, %.1f%%' % (i, num, percentage))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AOLe5T_l4vmO"
      },
      "outputs": [],
      "source": [
        "# delete columns where number of unique values is less than 1% of the rows\n",
        "from pandas import read_csv\n",
        "# define the location of the dataset\n",
        "path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/oil-spill.csv'\n",
        "# load the dataset\n",
        "df = read_csv(path, header=None)\n",
        "print(df.shape)\n",
        "# get number of unique values for each column\n",
        "counts = df.nunique()\n",
        "# record columns to delete\n",
        "to_del = [i for i,v in enumerate(counts) if (float(v)/df.shape[0]*100) < 1]\n",
        "print(to_del)\n",
        "# drop useless columns\n",
        "df.drop(to_del, axis=1, inplace=True)\n",
        "print(df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-BWyjkK5CNR",
        "outputId": "cffbbe11-a41e-4765-8ee1-f0955497070c"
      },
      "outputs": [],
      "source": [
        "# example of apply the variance threshold\n",
        "from pandas import read_csv\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "# define the location of the dataset\n",
        "path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/oil-spill.csv'\n",
        "# load the dataset\n",
        "df = read_csv(path, header=None)\n",
        "# split data into inputs and outputs\n",
        "data = df.values\n",
        "X = data[:, :-1]\n",
        "y = data[:, -1]\n",
        "print(X.shape, y.shape)\n",
        "# define the transform\n",
        "transform = VarianceThreshold()\n",
        "# transform the input data\n",
        "X_sel = transform.fit_transform(X)\n",
        "print(X_sel.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fDD419bU5PCC"
      },
      "outputs": [],
      "source": [
        "# explore the effect of the variance thresholds on the number of selected features\n",
        "from numpy import arange\n",
        "from pandas import read_csv\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from matplotlib import pyplot\n",
        "# define the location of the dataset\n",
        "path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/oil-spill.csv'\n",
        "# load the dataset\n",
        "df = read_csv(path, header=None)\n",
        "# split data into inputs and outputs\n",
        "data = df.values\n",
        "X = data[:, :-1]\n",
        "y = data[:, -1]\n",
        "print(X.shape, y.shape)\n",
        "# define thresholds to check\n",
        "thresholds = arange(0.0, 0.55, 0.05)\n",
        "# apply transform with each threshold\n",
        "results = list()\n",
        "for t in thresholds:\n",
        "\t# define the transform\n",
        "\ttransform = VarianceThreshold(threshold=t)\n",
        "\t# transform the input data\n",
        "\tX_sel = transform.fit_transform(X)\n",
        "\t# determine the number of input features\n",
        "\tn_features = X_sel.shape[1]\n",
        "\tprint('>Threshold=%.2f, Features=%d' % (t, n_features))\n",
        "\t# store the result\n",
        "\tresults.append(n_features)\n",
        "# plot the threshold vs the number of selected features\n",
        "pyplot.plot(thresholds, results)\n",
        "pyplot.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AeihnlDQ504v",
        "outputId": "d060803c-171e-4070-e9e7-4d2d9c53548d"
      },
      "outputs": [],
      "source": [
        "# locate rows of duplicate data\n",
        "from pandas import read_csv\n",
        "# define the location of the dataset\n",
        "path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/iris.csv'\n",
        "# load the dataset\n",
        "df = read_csv(path, header=None)\n",
        "# calculate duplicates\n",
        "dups = df.duplicated()\n",
        "# report if there are any duplicates\n",
        "print(dups.any())\n",
        "# list all duplicate rows\n",
        "print(df[dups])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svPAK-No58bn"
      },
      "outputs": [],
      "source": [
        "# delete rows of duplicate data from the dataset\n",
        "from pandas import read_csv\n",
        "# define the location of the dataset\n",
        "path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/iris.csv'\n",
        "# load the dataset\n",
        "df = read_csv(path, header=None)\n",
        "print(df.shape)\n",
        "# delete duplicate rows\n",
        "df.drop_duplicates(inplace=True)\n",
        "print(df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-_R_LP66cQy"
      },
      "source": [
        "**COMENTARIOS DEL EJERCICIO**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i10ri_NL6d9_"
      },
      "source": [
        "Aprendí de distintas funciones que tiene incorporada la librería pandas que ayudan a agilizar la limpieza del dataset, así como aprender a distinguir las distintas formas en las que se puede presentar una oportundiad de mejora con respecto a la calidad de un dataset. Como profesional, espero identificar correctamente qué técnicas emplear para limpiar y no echar a perder un buen set de datos."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "reporte_tutorial_2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
